# MMM Benchmarking Guide

A clear, step-by-step guide for using the benchmarking system to test different model configurations.

## Prerequisites

### Authentication

Set up your Google Cloud credentials:

```bash
# Use Application Default Credentials (this is what works!)
export GOOGLE_APPLICATION_CREDENTIALS=~/.config/gcloud/application_default_credentials.json
```

‚ö†Ô∏è **Important:** Keep the entire command on **ONE LINE** (no line breaks)! A line break after the `=` sign will cause a confusing "permission denied" error.

This uses the impersonated credentials file created by `gcloud auth application-default login`.

### Common Mistakes

**‚ùå WRONG - Line break causes "permission denied" error:**
```bash
export GOOGLE_APPLICATION_CREDENTIALS=
/path/to/file.json
```

The shell interprets this as two separate commands:
1. `export GOOGLE_APPLICATION_CREDENTIALS=` (sets variable to empty)
2. `/path/to/file.json` (tries to execute the file ‚Üí permission denied!)

**‚úÖ CORRECT - All on one line:**
```bash
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/file.json
```

### Verify Setup

```bash
# Check you can access GCS
gsutil ls gs://mmm-app-output/

# Check you can list Cloud Run jobs
gcloud run jobs list --region=europe-west1
```

## Quick Start

### 1. List Available Benchmarks

```bash
python scripts/benchmark_mmm.py --list-configs
```

This shows all pre-configured benchmarks in the `benchmarks/` directory with correct variant counts.

### 2. Validate Configuration (Recommended)

Before running expensive jobs, validate your configuration:

```bash
# Preview what would be generated (no jobs submitted)
python scripts/benchmark_mmm.py --config benchmarks/adstock_comparison.json --dry-run

# Quick test with minimal resources (10 iterations, 1 trial)
python scripts/benchmark_mmm.py --config benchmarks/adstock_comparison.json --test-run
```

### 3. Run a Benchmark

```bash
# Run adstock comparison benchmark
python scripts/benchmark_mmm.py --config benchmarks/adstock_comparison.json
```

This creates job configurations and adds them to the queue.

### 4. Process the Queue

```bash
# Process all pending jobs with automatic cleanup
python scripts/process_queue_simple.py --loop --cleanup
```

This launches the jobs on Cloud Run and processes them until the queue is empty. The `--cleanup` flag removes old completed jobs to keep the queue manageable.

## Complete Workflow Example

Here's a complete example testing different adstock types:

### Step 1: Review Available Benchmarks

```bash
ls benchmarks/
# adstock_comparison.json
# spend_var_mapping.json
# train_val_test_splits.json
# time_aggregation.json
# comprehensive_benchmark.json
```

### Step 2: Run Adstock Comparison

```bash
# First do a dry run to see what will be created
python scripts/benchmark_mmm.py \
  --config benchmarks/adstock_comparison.json \
  --dry-run

# Output shows 3 variants will be created:
# - geometric
# - weibull_cdf
# - weibull_pdf

# Now submit for real
python scripts/benchmark_mmm.py \
  --config benchmarks/adstock_comparison.json
```

### Step 3: Process the Jobs

```bash
# Set authentication
export GOOGLE_APPLICATION_CREDENTIALS=~/.config/gcloud/application_default_credentials.json

# Process all pending jobs
python scripts/process_queue_simple.py --loop
```

You'll see output like:
```
INFO - Loaded queue 'default-dev' from GCS
INFO - Total: 3
INFO - Pending: 3
INFO - Processing job 1/3
INFO - ‚úÖ Launched job: mmm-app-dev-training
...
INFO - ‚úÖ Processed 3 job(s)
```

### Step 4: Monitor Progress

Monitor jobs in the Cloud Console:
```
https://console.cloud.google.com/run/jobs/executions?project=datawarehouse-422511
```

Or via command line:
```bash
gcloud run jobs executions list \
  --job=mmm-app-dev-training \
  --region=europe-west1 \
  --limit=10
```

### Step 5: Collect Results

Once jobs complete (15-30 minutes each):

```bash
python scripts/benchmark_mmm.py \
  --collect-results adstock_comparison_20260211_120000 \
  --export-format csv
```

Results are saved to:
```
gs://mmm-app-output/benchmarks/adstock_comparison_20260211_120000/results.csv
```

### Step 6: Analyze Results

```python
import pandas as pd

# Load results
df = pd.read_csv('results.csv')

# Compare variants
summary = df.groupby('benchmark_variant')[['rsq_train', 'rsq_val', 'rsq_test', 'nrmse_val', 'decomp_rssd']].mean()
print(summary)

# Find best performer
best = df.loc[df['rsq_val'].idxmax()]
print(f"Best variant: {best['benchmark_variant']}")
print(f"Val R¬≤: {best['rsq_val']:.3f}")
print(f"Test R¬≤: {best['rsq_test']:.3f}")

# Visualize tradeoffs
import plotly.express as px
fig = px.scatter(df, x='nrmse_val', y='decomp_rssd', 
                 color='benchmark_variant',
                 hover_data=['rsq_val', 'rsq_test'])
fig.show()
```

## Validating Configurations

Before running expensive benchmarks, validate your configuration.

### Dry Run Mode

Preview what would be generated without submitting jobs:

```bash
python scripts/benchmark_mmm.py --config benchmarks/adstock_comparison.json --dry-run
```

Output shows:
```
üîç DRY RUN MODE - No jobs will be submitted

Benchmark: adstock_comparison
Description: Compare different adstock transformation types...

Would generate 3 variants:

Variant 1/3: geometric
  Test: adstock
  Adstock: geometric
  Hyperparameter preset: Meshed recommend
  Iterations: 2000
  Trials: 5

Variant 2/3: weibull_cdf
  ...
```

### Test Run Mode

Run a quick test with minimal resources to verify everything works:

```bash
python scripts/benchmark_mmm.py --config benchmarks/adstock_comparison.json --test-run
```

This:
- Reduces iterations to 10 (from 2000)
- Reduces trials to 1 (from 5)
- Submits only the first variant
- Allows quick validation before full run

Output shows:
```
üß™ TEST RUN MODE - Running first variant with minimal settings

Iterations: 10 (reduced from 2000)
Trials: 1 (reduced from 5)

Testing variant: geometric
‚úÖ Test job submitted to queue: default-dev
```

Then process the test job:
```bash
python scripts/process_queue_simple.py --count 1
```

## Benchmark Configuration

### JSON Structure

Create a benchmark configuration file:

```json
{
  "name": "my_test",
  "description": "Testing adstock types",
  "base_config": {
    "country": "de",
    "goal": "UPLOAD_VALUE",
    "version": "20251211_115528"
  },
  "iterations": 2000,
  "trials": 5,
  "max_combinations": 10,
  "variants": {
    "adstock": [
      {
        "name": "geometric",
        "type": "geometric",
        "hyperparameter_preset": "Meshed recommend"
      },
      {
        "name": "weibull_cdf",
        "type": "weibull_cdf",
        "hyperparameter_preset": "Meta default"
      }
    ]
  }
}
```

### Configuration Fields

- **name**: Identifier for this benchmark
- **description**: What you're testing
- **base_config**: Reference configuration
  - `country`: Country code (e.g., "de", "uk")
  - `goal`: Target variable (e.g., "UPLOAD_VALUE")
  - `version`: Timestamp of prepared training data
- **iterations**: Robyn iterations per variant (default: 2000)
- **trials**: Robyn trials per variant (default: 5)
- **max_combinations**: Limit total variants generated
- **combination_mode**: How to generate variants (see below)
- **variants**: Test specifications (see below)

### Combination Mode

Control how variants are generated when you have multiple variant types:

**"single" mode (default):**
- Generates variants for each dimension separately
- Example: 3 adstock types + 5 train splits = 8 total variants
- Use when testing one dimension at a time

**"cartesian" mode:**
- Generates all combinations of all dimensions
- Example: 3 adstock types √ó 5 train splits = 15 total variants  
- Use when testing interactions between dimensions

Example configuration:

```json
{
  "name": "comprehensive_test",
  "description": "Test all combinations",
  "combination_mode": "cartesian",
  "variants": {
    "adstock": [
      {"name": "geometric", "type": "geometric"},
      {"name": "weibull_cdf", "type": "weibull_cdf"}
    ],
    "train_splits": [
      {"name": "70_90", "train_size": [0.7, 0.9]},
      {"name": "75_90", "train_size": [0.75, 0.9]}
    ]
  }
}
```

This generates 4 combinations:
- geometric + 70_90 split
- geometric + 75_90 split
- weibull_cdf + 70_90 split
- weibull_cdf + 75_90 split

Use `--dry-run` to preview combinations before running!

## Test Types

### 1. Adstock Comparison

Test different adstock transformation types:

```json
{
  "variants": {
    "adstock": [
      {"name": "geometric", "type": "geometric"},
      {"name": "weibull_cdf", "type": "weibull_cdf"},
      {"name": "weibull_pdf", "type": "weibull_pdf"}
    ]
  }
}
```

**Evaluation focus:**
- Model fit (R¬≤, NRMSE)
- Decomposition quality (decomp.rssd)
- Channel-specific patterns

### 2. Train/Val/Test Splits

Test different split ratios:

```json
{
  "variants": {
    "train_splits": [
      {"name": "70_90", "train_size": [0.7, 0.9]},
      {"name": "75_90", "train_size": [0.75, 0.9]},
      {"name": "80_90", "train_size": [0.8, 0.9]}
    ]
  }
}
```

**Evaluation focus:**
- Val vs test performance gap
- Overfitting indicators
- Generalization capability

### 3. Time Aggregation

Compare different time granularities:

```json
{
  "variants": {
    "time_aggregation": [
      {"name": "daily", "frequency": "none"},
      {"name": "weekly", "frequency": "W"}
    ]
  }
}
```

**Evaluation focus:**
- Fit quality vs granularity tradeoff
- Allocator stability
- Budget optimization feasibility

### 4. Spend‚ÜíVar Mapping

Test different mapping strategies:

```json
{
  "variants": {
    "spend_var_mapping": [
      {"name": "all_spend", "type": "spend_to_spend"},
      {"name": "all_proxy", "type": "spend_to_proxy"}
    ]
  }
}
```

**Evaluation focus:**
- R¬≤ and NRMSE comparison
- ROAS by channel
- Coefficient stability

## Queue Processing

### Basic Usage

Process pending jobs in the queue:

```bash
# Process all pending jobs
python scripts/process_queue_simple.py --loop

# Process specific number of jobs
python scripts/process_queue_simple.py --count 3
```

### Queue Cleanup

The queue automatically tracks job completion status (PENDING ‚Üí RUNNING ‚Üí COMPLETED/FAILED). Use cleanup to remove old jobs:

```bash
# Process with automatic cleanup
python scripts/process_queue_simple.py --loop --cleanup

# Keep only 10 most recent completed jobs
python scripts/process_queue_simple.py --loop --cleanup --keep-completed 10
```

Without cleanup, the queue accumulates old completed jobs. The `--cleanup` flag removes them while keeping a configurable number of recent ones for reference.

### Result Path Logging

When jobs are launched, the script logs where results will be saved:

```
Processing job 1/3
  Country: de
  Revision: default
  Job ID: abc123

üìÇ Results will be saved to:
   gs://mmm-app-output/robyn/default/de/20260212_093045_123/
   Key files: model_summary.json, best_model_plots.png, console.log

‚úÖ Job launched successfully
   Execution ID: projects/.../executions/...

üí° To check results when job completes:
   gsutil ls gs://mmm-app-output/robyn/default/de/20260212_093045_123/
   gsutil cat gs://mmm-app-output/robyn/default/de/20260212_093045_123/model_summary.json
```

Each job gets a unique timestamp (with milliseconds) to prevent result overwrites.

### Queue Status

Monitor queue status:

```bash
üìä Queue Status: default-dev
  Total: 25
  Pending: 3
  Running: 2
  Completed: 18
  Failed: 2
```

The script automatically updates job statuses by checking Cloud Run execution status.

## Results Analysis

### Metrics Collected

Each benchmark variant produces:

**Model Fit:**
- `rsq_train`, `rsq_val`, `rsq_test` - R¬≤ scores
- `nrmse_train`, `nrmse_val`, `nrmse_test` - Normalized RMSE
- `mape` - Mean absolute percentage error

**Decomposition:**
- `decomp_rssd` - Decomposition stability metric

**Business Metrics:**
- `channel_roas` - ROAS per channel (JSON)
- `channel_contributions` - Contribution percentages

**Metadata:**
- `training_time_mins` - Execution time
- `status` - Job completion status
- `pareto_model_count` - Number of Pareto front models

### Analysis Example

```python
import pandas as pd
import plotly.express as px

# Load results
df = pd.read_csv('gs://mmm-app-output/benchmarks/test_id/results.csv')

# Basic comparison
print("\nMean metrics by variant:")
print(df.groupby('benchmark_variant')[['rsq_val', 'nrmse_val', 'decomp_rssd']].mean())

# Find best model
best = df.nlargest(1, 'rsq_val')
print(f"\nBest model: {best['benchmark_variant'].values[0]}")
print(f"Val R¬≤: {best['rsq_val'].values[0]:.3f}")
print(f"Test R¬≤: {best['rsq_test'].values[0]:.3f}")

# Visualize tradeoffs
fig = px.scatter(df, 
                 x='nrmse_val', 
                 y='decomp_rssd',
                 color='benchmark_variant',
                 size='rsq_val',
                 hover_data=['rsq_test', 'training_time_mins'])
fig.update_layout(title='Model Performance Tradeoffs')
fig.show()

# Check consistency across metrics
fig2 = px.scatter(df,
                  x='rsq_val',
                  y='rsq_test',
                  color='benchmark_variant',
                  hover_data=['nrmse_val', 'decomp_rssd'])
fig2.add_shape(type='line', x0=0, y0=0, x1=1, y1=1, 
               line=dict(dash='dash', color='gray'))
fig2.update_layout(title='Validation vs Test Performance')
fig2.show()
```

## Common Use Cases

### Use Case 1: Test Adstock Types

**Goal:** Determine which adstock type works best for your data.

```bash
# Run comparison
python scripts/benchmark_mmm.py --config benchmarks/adstock_comparison.json
python scripts/process_queue_simple.py --loop

# Wait for completion, then analyze
python -c "
import pandas as pd
df = pd.read_csv('results.csv')
print(df.groupby('benchmark_variant')[['rsq_val', 'decomp_rssd']].mean())
"
```

### Use Case 2: Find Optimal Train/Val/Test Split

**Goal:** Find the split ratio that balances fit quality and generalization.

```bash
# Run split tests
python scripts/benchmark_mmm.py --config benchmarks/train_val_test_splits.json
python scripts/process_queue_simple.py --loop

# Analyze val vs test gap
python -c "
import pandas as pd
df = pd.read_csv('results.csv')
df['val_test_gap'] = df['rsq_val'] - df['rsq_test']
print(df[['benchmark_variant', 'rsq_val', 'rsq_test', 'val_test_gap']].sort_values('val_test_gap'))
"
```

### Use Case 3: Daily vs Weekly Aggregation

**Goal:** Determine if weekly aggregation improves stability.

```bash
# Run time aggregation test
python scripts/benchmark_mmm.py --config benchmarks/time_aggregation.json
python scripts/process_queue_simple.py --loop

# Compare stability
python -c "
import pandas as pd
df = pd.read_csv('results.csv')
print(df.groupby('benchmark_variant')[['rsq_val', 'decomp_rssd']].agg(['mean', 'std']))
"
```

### Use Case 4: Spend‚ÜíSpend vs Spend‚ÜíProxy

**Goal:** Test if using proxy variables (sessions, clicks) is better than spend.

```bash
# Run mapping test
python scripts/benchmark_mmm.py --config benchmarks/spend_var_mapping.json
python scripts/process_queue_simple.py --loop

# Compare ROAS patterns
python -c "
import pandas as pd
import json
df = pd.read_csv('results.csv')
for _, row in df.iterrows():
    print(f\"\\n{row['benchmark_variant']}:\")
    roas = json.loads(row['channel_roas'])
    for channel, value in roas.items():
        print(f\"  {channel}: {value:.2f}\")
"
```

## Best Practices

### 1. Always Validate First

- Use `--dry-run` to preview what will be generated
- Use `--test-run` for quick validation before full runs
- Check variant counts match expectations

```bash
# Recommended workflow
python scripts/benchmark_mmm.py --config FILE --dry-run    # Preview
python scripts/benchmark_mmm.py --config FILE --test-run   # Test
python scripts/benchmark_mmm.py --config FILE              # Full run
```

### 2. Start Small

- Test with 2-3 variants first
- Use lower iterations (1000) for initial exploration
- Set reasonable `max_combinations` limits

### 3. Use Queue Cleanup

- Always use `--cleanup` flag to manage queue
- Old completed jobs clutter the queue
- Keep 10-20 recent completed jobs for reference

```bash
python scripts/process_queue_simple.py --loop --cleanup --keep-completed 10
```

### 4. Monitor Result Paths

- Check logged result paths when jobs launch
- Note the unique timestamp for each job
- Results are logged as: `gs://bucket/robyn/revision/country/timestamp/`

### 5. Incremental Testing

- Test one dimension at a time (use "single" mode)
- Use results to inform next tests
- Build up knowledge systematically

### 6. Use Combinations Wisely

- Use `combination_mode: "cartesian"` only when needed
- Combinations grow multiplicatively (2 √ó 3 √ó 4 = 24 variants)
- Always preview with `--dry-run` first

### 7. Monitor Resources

- Each variant = full training job (15-30 min)
- 10 variants √ó 20 min = 3+ hours compute
- Plan accordingly for costs

### 8. Interpret Carefully

- Look for consistent patterns, not single "best" results
- Consider multiple metrics (fit + stability + business)
- Validate on multiple datasets if possible

### 9. Document Findings

- Save configurations and results
- Note which patterns generalize
- Build institutional knowledge

## Troubleshooting

### Authentication Issues

If you get permission errors:

```bash
# Ensure credentials are set
export GOOGLE_APPLICATION_CREDENTIALS=~/.config/gcloud/application_default_credentials.json

# Verify they work
gcloud auth application-default print-access-token
```

### Queue Not Processing

Check queue status:

```bash
# View queue directly
gsutil cat gs://mmm-app-output/robyn-queues/default-dev/queue.json | jq .

# Check for paused status
python -c "
import json
from google.cloud import storage
client = storage.Client()
bucket = client.bucket('mmm-app-output')
blob = bucket.blob('robyn-queues/default-dev/queue.json')
queue = json.loads(blob.download_as_text())
print(f\"Status: {queue.get('status', 'active')}\")
print(f\"Entries: {len(queue.get('entries', []))}\")
"
```

### Jobs Failing

Check job logs:

```bash
# Get recent executions
gcloud run jobs executions list \
  --job=mmm-app-dev-training \
  --region=europe-west1 \
  --limit=5

# Get logs for specific execution
gcloud run jobs executions describe EXECUTION_ID \
  --job=mmm-app-dev-training \
  --region=europe-west1
```

### Results Not Found

If `--collect-results` finds 0 results even though jobs completed successfully:

**Check where results are saved:**

```bash
# Show expected locations
python scripts/benchmark_mmm.py --show-results-location BENCHMARK_ID

# List available results
python scripts/benchmark_mmm.py --list-results BENCHMARK_ID
```

Results are saved at:
```
gs://mmm-app-output/robyn/{revision}/{country}/{timestamp}/
```

For example:
```
gs://mmm-app-output/robyn/default/de/20260211_182030/
‚îú‚îÄ‚îÄ model_summary.json       # Metrics and metadata
‚îú‚îÄ‚îÄ best_model_plots.png     # Visualizations
‚îú‚îÄ‚îÄ model_params.json        # Configuration
‚îî‚îÄ‚îÄ ...
```

**Manual access:**

```bash
# List all results for a country
gsutil ls gs://mmm-app-output/robyn/default/de/

# View a specific model summary
gsutil cat gs://mmm-app-output/robyn/default/de/20260211_182030/model_summary.json | jq .

# Download all results
gsutil -m cp -r gs://mmm-app-output/robyn/default/de/20260211_*/ ./results/
```

**Why collection might not work:**

The R training scripts don't currently save benchmark metadata (benchmark_id, benchmark_test, benchmark_variant) to model_summary.json. This makes automatic matching unreliable. Use the manual access methods above to retrieve your results.

## Where Results Are Saved

### Result Structure

Each training job saves its results to GCS:

```
gs://mmm-app-output/robyn/{revision}/{country}/{timestamp}/
```

**Path components:**
- `revision`: Model revision (usually "default")
- `country`: Country code (e.g., "de", "uk", "us")
- `timestamp`: Job execution timestamp (YYYYMMDD_HHMMSS)

### Finding Your Results

**Option 1: Use --list-results**

```bash
python scripts/benchmark_mmm.py --list-results adstock_comparison_20260211_181644
```

Shows all model results that might match your benchmark.

**Option 2: Use --show-results-location**

```bash
python scripts/benchmark_mmm.py --show-results-location adstock_comparison_20260211_181644
```

Shows expected paths and provides gsutil commands.

**Option 3: Manual search**

```bash
# List all results for a country
gsutil ls gs://mmm-app-output/robyn/default/de/

# Results are timestamped directories
# Look for timestamps around when you submitted the benchmark
gsutil ls gs://mmm-app-output/robyn/default/de/20260211_18*/
```

### What's in Each Result

Each result directory contains:

- `model_summary.json` - Model metrics (rsq_val, nrmse_val, ROAS, etc.)
- `best_model_plots.png` - Visualizations of best model
- `model_params.json` - Configuration used for training
- `pareto_models.json` - All Pareto-optimal models
- Other Robyn output files

### Accessing Results

**View model summary:**

```bash
gsutil cat gs://mmm-app-output/robyn/default/de/20260211_182030/model_summary.json
```

**Download everything:**

```bash
# Download a specific result
gsutil -m cp -r gs://mmm-app-output/robyn/default/de/20260211_182030/ ./my_results/

# Download all results from a time range
gsutil -m cp -r gs://mmm-app-output/robyn/default/de/20260211_*/ ./all_results/
```

**Analyze in Python:**

```python
import json
from google.cloud import storage

client = storage.Client()
bucket = client.bucket('mmm-app-output')

# Load model summary
blob = bucket.blob('robyn/default/de/20260211_182030/model_summary.json')
summary = json.loads(blob.download_as_text())

# Print key metrics
best_model = summary['best_model']
print(f"R¬≤ (val): {best_model['rsq_val']:.3f}")
print(f"NRMSE (val): {best_model['nrmse_val']:.3f}")
print(f"MAPE: {best_model['mape']:.3f}")
```

## Next Steps

- Review example benchmarks in `benchmarks/` directory
- Check detailed documentation in `benchmarks/README.md`
- See workflow examples in `benchmarks/WORKFLOW_EXAMPLE.md`
- Explore architecture in `ARCHITECTURE.md`

## Summary

This guide covers:
- ‚úÖ Authentication setup (what actually works)
- ‚úÖ Running benchmarks to test configurations
- ‚úÖ Processing the job queue
- ‚úÖ Analyzing and comparing results
- ‚úÖ Common use cases with examples

You can now systematically test different MMM configurations to find what works best for your data!
